---
title: "Spark_ML"
author: "Bakro"
date: "1/3/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(sparklyr)
library(ggplot2)
library(dplyr)
```

```{r}
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jdk1.8.0_20")
sc <- spark_connect(master = "local")
```

## Algorithms

| **Function**                                                                                               | **Description**               |
|:-----------------------------------------------------------------------------------------------------------|:------------------------------|
| [`ml_kmeans`](https://spark.rstudio.com/reference/ml_kmeans)                                               | K-Means Clustering            |
| [`ml_linear_regression`](https://spark.rstudio.com/reference/ml_linear_regression)                         | Linear Regression             |
| [`ml_logistic_regression`](https://spark.rstudio.com/reference/ml_logistic_regression)                     | Logistic Regression           |
| [`ml_survival_regression`](https://spark.rstudio.com/reference/ml_survival_regression)                     | Survival Regression           |
| [`ml_generalized_linear_regression`](https://spark.rstudio.com/reference/ml_generalized_linear_regression) | Generalized Linear Regression |
| [`ml_decision_tree`](https://spark.rstudio.com/reference/ml_decision_tree)                                 | Decision Trees                |
| [`ml_random_forest`](https://spark.rstudio.com/reference/ml_random_forest)                                 | Random Forests                |
| [`ml_gradient_boosted_trees`](https://spark.rstudio.com/reference/ml_gradient_boosted_trees)               | Gradient-Boosted Trees        |
| [`ml_pca`](https://spark.rstudio.com/reference/ml_pca)                                                     | Principal Components Analysis |
| [`ml_naive_bayes`](https://spark.rstudio.com/reference/ml_naive_bayes)                                     | Naive-Bayes                   |
| [`ml_multilayer_perceptron`](https://spark.rstudio.com/reference/ml_multilayer_perceptron)                 | Multilayer Perceptron         |
| [`ml_lda`](https://spark.rstudio.com/reference/ml_lda)                                                     | Latent Dirichlet Allocation   |
| [`ml_one_vs_rest`](https://spark.rstudio.com/reference/ml_lda)                                             | One vs Rest                   |

```{r}
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)
iris_tbl
```

## Regression

### LINEAR REGRESSION

```{r}
mtcars_tbl <- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions <- mtcars_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

mtcars_training <- partitions$training
mtcars_test <- partitions$test

lm_model <- mtcars_training %>%
  ml_linear_regression(mpg ~ .)

pred <- ml_predict(lm_model, mtcars_test)

ml_regression_evaluator(pred, label_col = "mpg")
```

### Generalized Linear Regression

```{r}
# partitions <- mtcars_tbl %>%
#   sdf_random_split(training = 0.7, test = 0.3, seed = 1111)
# 
# mtcars_training <- partitions$training
# mtcars_test <- partitions$test

# Specify the grid
family <- c("gaussian", "gamma", "poisson")
link <- c("identity", "log")
family_link <- expand.grid(family = family, link = link, stringsAsFactors = FALSE)
family_link <- data.frame(family_link, rmse = 0)

# Train the models
for (i in seq_len(nrow(family_link))) {
  glm_model <- mtcars_training %>%
    ml_generalized_linear_regression(mpg ~ .,
      family = family_link[i, 1],
      link = family_link[i, 2]
    )

  pred <- ml_predict(glm_model, mtcars_test)
  family_link[i, 3] <- ml_regression_evaluator(pred, label_col = "mpg")
}

family_link
```

### Survival Regression

Fit a parametric survival regression model named accelerated failure time (AFT) model (see Accelerated failure time model (Wikipedia)) based on the Weibull distribution of the survival time.

```{r}
library(survival)

ovarian_tbl <- sdf_copy_to(sc, ovarian, name = "ovarian_tbl", overwrite = TRUE)

partitions <- ovarian_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

ovarian_training <- partitions$training
ovarian_test <- partitions$test

sur_reg <- ovarian_training %>%
  ml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = "fustat")

pred <- ml_predict(sur_reg, ovarian_test)
pred
```

### Isotonic Regression

Currently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported.

```{r}
# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
partitions <- iris_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training <- partitions$training
iris_test <- partitions$test

iso_res <- iris_tbl %>%
  ml_isotonic_regression(Petal_Length ~ Petal_Width)

pred <- ml_predict(iso_res, iris_test)

pred
```

## classification

### Linear SVM

Perform classification using linear support vector machines (SVM). This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. Only supports L2 regularization currently.

```{r}

# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
partitions <- iris_tbl %>%
  filter(Species != "setosa") %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training <- partitions$training
iris_test <- partitions$test

svc_model <- iris_training %>%
  ml_linear_svc(Species ~ .)

pred <- ml_predict(svc_model, iris_test)

ml_binary_classification_evaluator(pred)
```

### LOGISTIC REGRESSION

```{r}
# mtcars_tbl <- sdf_copy_to(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)
# 
# partitions <- mtcars_tbl %>%
#   sdf_random_split(training = 0.7, test = 0.3, seed = 1111)
# 
# mtcars_training <- partitions$training
# mtcars_test <- partitions$test

lr_model <- mtcars_training %>%
  ml_logistic_regression(am ~ gear + carb)

pred <- ml_predict(lr_model, mtcars_test)

ml_binary_classification_evaluator(pred)
```

### Naive Bayes Classifiers

```{r}
# Naive Bayes Classifiers. It supports Multinomial NB (see here) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see here). The input feature values must be nonnegative.

# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
# partitions <- iris_tbl %>%
#   sdf_random_split(training = 0.7, test = 0.3, seed = 1111)
# 
# iris_training <- partitions$training
# iris_test <- partitions$test

nb_model <- iris_training %>%
  ml_naive_bayes(Species ~ .)

pred <- ml_predict(nb_model, iris_test)

ml_multiclass_classification_evaluator(pred)
```

### multilayer_percepton_classifier

Classification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax.

```{r}
iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions <- iris_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training <- partitions$training
iris_test <- partitions$test

mlp_model <- iris_training %>%
  ml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3))

pred <- ml_predict(mlp_model, iris_test)

ml_multiclass_classification_evaluator(pred)
```

### OneVsRest

Reduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example.

```{r}
# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
partitions <- iris_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training <- partitions$training
iris_test <- partitions$test

one_vs_rest_model <- iris_training %>%
  ml_one_vs_rest(Species ~ .)

pred <- ml_predict(nb_model, iris_test)

ml_multiclass_classification_evaluator(pred)
```

## Tree

### Decision Trees

```{r}
#iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions <- iris_tbl %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

iris_training <- partitions$training
iris_test <- partitions$test

dt_model <- iris_training %>%
  ml_decision_tree(Species ~ .)

pred <- ml_predict(dt_model, iris_test)

ml_multiclass_classification_evaluator(pred)
```

### Gradient Boosted Trees

```{r}
# Perform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet.

# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
# partitions <- iris_tbl %>%
#   sdf_random_split(training = 0.7, test = 0.3, seed = 1111)
# 
# iris_training <- partitions$training
# iris_test <- partitions$test

gbt_model <- iris_training %>%
  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)

pred <- ml_predict(gbt_model, iris_test)

ml_regression_evaluator(pred, label_col = "Sepal_Length")
```

### Random Forest

```{r}
# iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)
# 
# partitions <- iris_tbl %>%
#   sdf_random_split(training = 0.7, test = 0.3, seed = 1111)
# 
# iris_training <- partitions$training
# iris_test <- partitions$test

rf_model <- iris_training %>%
  ml_random_forest(Species ~ ., type = "classification")

pred <- ml_predict(rf_model, iris_test)

ml_multiclass_classification_evaluator(pred)
```

## Clustering

### Bisecting K-Means Clustering

A bisecting k-means algorithm based on the paper "A comparison of document clustering techniques" by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority.

```{r}
iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %>%
  select(-Species) %>%
  ml_bisecting_kmeans(k = 4, Species ~ .)
```

### K-MEANS CLUSTERING

```{r}
kmeans_model <- iris_tbl %>%
  ml_kmeans(k = 3, features = c("Petal_Length", "Petal_Width"))
kmeans_model
```

```{r}
predicted <- ml_predict(kmeans_model, iris_tbl) %>%
  collect
table(predicted$Species, predicted$prediction)
```

```{r}
# plot cluster membership
ml_predict(kmeans_model) %>%
  collect() %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
             size = 2, alpha = 0.5) + 
  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
             col = scales::muted(c("red", "green", "blue")),
             pch = 'x', size = 12) +
  scale_color_discrete(name = "Predicted Cluster",
                       labels = paste("Cluster", 1:3)) +
  labs(
    x = "Petal Length",
    y = "Petal Width",
    title = "K-Means Clustering",
    subtitle = "Use Spark.ML to predict cluster membership with the iris dataset."
  )
```

### Gaussian Mixture clustering.

This class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated "mixing" weights specifying each's contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than tol, or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum.

```{r}

#iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

gmm_model <- ml_gaussian_mixture(iris_tbl, Species ~ .)
pred <- sdf_predict(iris_tbl, gmm_model)
ml_clustering_evaluator(pred)
```

### Latent Dirichlet Allocation Latent Dirichlet Allocation (LDA),

a topic model designed for text documents.

```{r}
library(janeaustenr)
library(dplyr)


lines_tbl <- sdf_copy_to(sc,
  austen_books()[c(1:30), ],
  name = "lines_tbl",
  overwrite = TRUE
)

# transform the data in a tidy form
lines_tbl_tidy <- lines_tbl %>%
  ft_tokenizer(
    input_col = "text",
    output_col = "word_list"
  ) %>%
  ft_stop_words_remover(
    input_col = "word_list",
    output_col = "wo_stop_words"
  ) %>%
  mutate(text = explode(wo_stop_words)) %>%
  filter(text != "") %>%
  select(text, book)

lda_model <- lines_tbl_tidy %>%
  ml_lda(~text, k = 4)

# vocabulary and topics
tidy(lda_model)
```

## recommendations

### als

Perform recommendation using Alternating Least Squares (ALS) matrix factorization. ml_recommend() returns the top n users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly.

```{r}
movies <- data.frame(
  user   = c(1, 2, 0, 1, 2, 0),
  item   = c(1, 1, 1, 2, 2, 0),
  rating = c(3, 1, 2, 4, 5, 4)
)
movies_tbl <- sdf_copy_to(sc, movies)

model <- ml_als(movies_tbl, rating ~ user + item)

ml_predict(model, movies_tbl)

ml_recommend(model, type = "item", 1)
ml_recommend(model, type = "user", 1)
```

### FT STRING INDEXING

Use `ft_string_indexer` and `ft_index_to_string` to convert a character column into a numeric column and back again

```{r}
ft_string2idx <- iris_tbl %>%
  ft_string_indexer("Species", "Species_idx") %>%
  ft_index_to_string("Species_idx", "Species_remap") %>%
  collect

table(ft_string2idx$Species, ft_string2idx$Species_remap)
```

## others

### save and load model

```{r eval=FALSE}
ml_save(x, path, overwrite = FALSE, ...) # x is ml_model
ml_load(sc, path)  # sc spark connection 
```

### data associated

Extracts data associated with a Spark ML model

```{r eval =FALSE}
ml_model_data(object) #  object is Ml_model
```

### PCA

```{r}
pca_model <- tbl(sc, "iris") %>%
  select(-Species) %>%
  ml_pca()
print(pca_model)
```

### Chi-square hypothesis testing

```{r}
iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features <- c("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_chisquare_test(iris_tbl, features = features, label = "Species")
```

### Compute correlation matrix

```{r}
#iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

features <- c("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_corr(iris_tbl, columns = features, method = "pearson")
```

### tuning

Perform hyper-parameter tuning using either K-fold cross validation or train-validation split

```{r}
#iris_tbl <- sdf_copy_to(sc, iris, name = "iris_tbl", overwrite = TRUE)

# Create a pipeline
pipeline <- ml_pipeline(sc) %>%
  ft_r_formula(Species ~ .) %>%
  ml_random_forest_classifier()

# Specify hyperparameter grid
grid <- list(
  random_forest = list(
    num_trees = c(5, 10),
    max_depth = c(5, 10),
    impurity = c("entropy", "gini")
  )
)

# Create the cross validator object
cv <- ml_cross_validator(
  sc,
  estimator = pipeline, estimator_param_maps = grid,
  evaluator = ml_multiclass_classification_evaluator(sc),
  num_folds = 3,
  parallelism = 4
)

# Train the models
cv_model <- ml_fit(cv, iris_tbl)

# Print the metrics
ml_validation_metrics(cv_model)
```

### Default stop words

```{r}
# ml_default_stop_words(
#   sc,
#   language = c("english", "danish", "dutch", "finnish", "french", "german",
#     "hungarian", "italian", "norwegian", "portuguese", "russian", "spanish", "swedish",
#     "turkish"),
#   ...
# )
ml_default_stop_words(sc, language = "english")
```
